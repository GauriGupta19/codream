{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50732264-d0ba-4e3a-bfe8-b274d9cdf1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "import PIL.Image\n",
    "import torch.cuda.amp as amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45cc55b2-03ac-448e-9803-039c7409700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c22f5fc-0414-4b05-81b7-0e4a1d49707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.cifar import CIFAR10\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a154bc4-247b-4088-8521-0d35858c2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6afe805d-b383-4dbe-94ad-4f0698f4e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_ids = [3, 2, 0]\n",
    "batch_size = len(device_ids) * 256\n",
    "device = torch.device('cuda:{}'.format(device_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a631b310-6ce1-483c-8468-e7f80d4438d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 32\n",
    "num_classes = 10\n",
    "\n",
    "train_transform = T.Compose(\n",
    "    [\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), \n",
    "            (0.2023, 0.1994, 0.2010)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "test_transform = T.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), \n",
    "            (0.2023, 0.1994, 0.2010)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_dataset = CIFAR10(\n",
    "    root=\"./imgs/cifar10\", train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_dataset = CIFAR10(\n",
    "    root=\"./imgs/cifar10\", train=False, download=True, transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "884e5291-9781-4f4d-be0b-d484f94c4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21cc3e34-9962-4f2f-ae12-92e386398c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6daaf69e-ec62-4e80-84d6-95e08d663baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam\n",
    "\n",
    "c_models = []\n",
    "c_optims = []\n",
    "c_dsets = []\n",
    "c_dloaders = []\n",
    "\n",
    "num_clients = 2\n",
    "samples_per_client = 49000\n",
    "\n",
    "for i in range(num_clients):\n",
    "    c_model = nn.DataParallel(ResNet34().to(device), device_ids=device_ids)\n",
    "    c_optim = optim(c_model.parameters(), lr=3e-4)\n",
    "    c_idx = indices[i*samples_per_client: (i+1)*samples_per_client]\n",
    "    c_dset = Subset(train_dataset, c_idx)\n",
    "    c_dloader = DataLoader(c_dset, batch_size=64*len(device_ids), shuffle=True)\n",
    "\n",
    "    c_models.append(c_model)\n",
    "    c_optims.append(c_optim)\n",
    "    c_dsets.append(c_dset)\n",
    "    c_dloaders.append(c_dloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63dfc7-04b6-4b30-a8e2-83b40bcedb49",
   "metadata": {},
   "source": [
    "## Isolated Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "587fb34c-c573-4f64-8436-9287f7ef5b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, client 1, acc 0.3383\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'acc_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     acc \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, client \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, acc \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, client_num, acc))\n\u001b[0;32m---> 18\u001b[0m     \u001b[43macc_list\u001b[49m[\u001b[38;5;28mstr\u001b[39m(client_num)]\u001b[38;5;241m.\u001b[39mappend(acc\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_clients):\n\u001b[1;32m     21\u001b[0m     client_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acc_list' is not defined"
     ]
    }
   ],
   "source": [
    "total_epochs = 200\n",
    "# fig, axs = plt.subplots(ncols=num_clients, nrows=1, figsize=(22, 3))\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    for client_num in range(num_clients):\n",
    "        client_num = 1\n",
    "        c_model = c_models[client_num]\n",
    "        c_model.eval()\n",
    "        correct, total = 0, 0\n",
    "        for x, y_hat in test_loader:\n",
    "            with torch.no_grad():\n",
    "                x, y_hat = x.to(device), y_hat.to(device)\n",
    "                y = c_model(x, layer_num=0)\n",
    "                correct += (y.argmax(dim=1) == y_hat).sum()\n",
    "                total += x.shape[0]\n",
    "        acc = correct / total\n",
    "        print(\"epoch {}, client {}, acc {:.4f}\".format(epoch, client_num, acc))\n",
    "        acc_list[str(client_num)].append(acc.item())\n",
    "\n",
    "    for client_num in range(num_clients):\n",
    "        client_num = 1\n",
    "        c_model = c_models[client_num]\n",
    "        c_model.train()\n",
    "        c_optim = c_optims[client_num]\n",
    "        c_dloader = c_dloaders[client_num]\n",
    "        for x, y_hat in c_dloader:\n",
    "            x, y_hat = x.to(device), y_hat.to(device)\n",
    "            c_optim.zero_grad()\n",
    "            y = c_model(x, layer_num=0)\n",
    "            loss = loss_fn(y, y_hat)\n",
    "            loss.backward()\n",
    "            c_optim.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f5c00-84d2-4799-869b-1f6b1eca4cc0",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc8f482a-07df-4256-b60c-3c8c090b3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clients):\n",
    "    torch.save(c_models[i].module.state_dict(), f\"./expt_dump/c{i}_isolated.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4519fe-b2ef-420a-95aa-691063102205",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb2aa387-71ba-405f-bcb2-712cbef101dc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './saved_models/c0_isolated.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_clients):\n\u001b[0;32m----> 2\u001b[0m     c_models[i]\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./saved_models/c\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_isolated.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/serialization.py:710\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    708\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 710\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    715\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/serialization.py:240\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/serialization.py:221\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './saved_models/c0_isolated.pt'"
     ]
    }
   ],
   "source": [
    "for i in range(num_clients):\n",
    "    c_models[i].module.load_state_dict(torch.load(f\"./expt_dump/c{i}_isolated.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551de768-9b44-4335-871a-9f6cdad38ad0",
   "metadata": {},
   "source": [
    "## Evaluate on a common test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a7c7671-ee3e-4a88-ba8a-41359b9e1f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client 0, acc 0.9122\n",
      "client 1, acc 0.3383\n"
     ]
    }
   ],
   "source": [
    "for client_num in range(num_clients):\n",
    "    c_model = c_models[client_num]\n",
    "    c_model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x, y_hat in test_loader:\n",
    "        with torch.no_grad():\n",
    "            x, y_hat = x.to(device), y_hat.to(device)\n",
    "            y = c_model(x)\n",
    "            correct += (y.argmax(dim=1) == y_hat).sum()\n",
    "            total += x.shape[0]\n",
    "    print(\"client {}, acc {:.4f}\".format(client_num, correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01482ac-510a-4d73-8acb-2b4500b8a0d6",
   "metadata": {},
   "source": [
    "## Collaborative Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4951d95-8814-4613-841d-5e7d156c8fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import DeepInversionFeatureHook, total_variation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cf4bb12-0a05-4c78-8631-1bee6532c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=np.array((0.4914, 0.4822, 0.4465))\n",
    "std=np.array((0.2023, 0.1994, 0.2010))\n",
    "\n",
    "LOWER_IMAGE_BOUND = torch.tensor((-mean / std).reshape(1, -1, 1, 1)).float().to(device)\n",
    "UPPER_IMAGE_BOUND = torch.tensor(((1 - mean) / std).reshape(1, -1, 1, 1)).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd878229-b9fc-4252-bfa7-198a07a708d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tv_loss(inputs_jit):\n",
    "    diff1 = inputs_jit[:,:,:,:-1] - inputs_jit[:,:,:,1:]\n",
    "    diff2 = inputs_jit[:,:,:-1,:] - inputs_jit[:,:,1:,:]\n",
    "    diff3 = inputs_jit[:,:,1:,:-1] - inputs_jit[:,:,:-1,1:]\n",
    "    diff4 = inputs_jit[:,:,:-1,:-1] - inputs_jit[:,:,1:,1:]\n",
    "    loss_var = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n",
    "    return loss_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a1bfb72e-5ac3-4a19-b2b0-c46241b9f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grad_ascent_on_data(inp, target_label, model, position,\n",
    "                            img_model=None,\n",
    "                            alpha_ce=1.0, alpha_f=1.0, alpha_tv=3e-5, alpha_l2=3e-8,\n",
    "                            lr=3e-2, steps=2000):\n",
    "\n",
    "    loss_r_feature_layers = []\n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            loss_r_feature_layers.append(DeepInversionFeatureHook(module))\n",
    "\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # inp_noise = inp.detach().clone()\n",
    "    updated_img = inp.detach()#.clone()\n",
    "    updated_img.requires_grad = True\n",
    "    updated_img.retain_grad()\n",
    "\n",
    "    if img_model is None:\n",
    "        optimizer = torch.optim.Adam([updated_img], lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(img_model.parameters(), lr=lr, eps=1e-8)\n",
    "    lim_0, lim_1 = 2, 2\n",
    "    for it in range(steps):\n",
    "        off1 = random.randint(-lim_0, lim_0)\n",
    "        off2 = random.randint(-lim_1, lim_1)\n",
    "        # if img_model is not None:\n",
    "        #     for n in [x for x in img_model.parameters() if len(x) == 4]:\n",
    "        #         n = n + n.detach().clone().normal_()*n.std()/50\n",
    "        #     updated_img = img_model(orig_img)\n",
    "        inputs_jit = torch.roll(updated_img, shifts=(off1, off2), dims=(2,3))\n",
    "        # inputs_jit = updated_img\n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        acts = model.module(inputs_jit, position)\n",
    "        ce_loss = loss_fn(acts, target_label)\n",
    "        # ce_loss = nn.MSELoss()(acts, target_label)\n",
    "\n",
    "        loss_r_feature = sum([model.r_feature for (idx, model) in enumerate(loss_r_feature_layers)])\n",
    "        loss = alpha_ce * ce_loss + alpha_tv * tv_loss(updated_img) + alpha_l2 * torch.linalg.norm(updated_img) + alpha_f * loss_r_feature\n",
    "\n",
    "        if it % 50 == 0:\n",
    "            acc = (acts.argmax(dim=1) == target_label).sum() / acts.shape[0]\n",
    "            print(f\"{it}/{steps}\", ce_loss.item(), loss_r_feature.item(), acc)\n",
    "            # print(acts[0], target_label[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if img_model is None:\n",
    "            updated_img.grad.data.zero_()\n",
    "            # updated_img.data = torch.clamp(updated_img.data, min=LOWER_IMAGE_BOUND, max=UPPER_IMAGE_BOUND)\n",
    "\n",
    "    for item in loss_r_feature_layers:\n",
    "        item.close()\n",
    "    # print(\"FINAL max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "    # del loss_r_feature_layers, acts, grads, inputs_jit, updated_img\n",
    "    # torch.cuda.empty_cache()\n",
    "    # print(\"AFTER memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    # print(\"AFTER memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    # print(\"AFTER max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    # print(time.time() - start, \"seconds\")\n",
    "    return updated_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ec4d171-758b-4fa8-ab30-79f380dc5f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_acc_list = {}\n",
    "for i in range(num_clients):\n",
    "    new_acc_list[str(i)] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e61a8824-a18c-4f4b-a22f-2289b0cbf9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [], '1': []}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f3592-9b71-4aa5-b1f4-794117c99c94",
   "metadata": {},
   "source": [
    "# Check performance improvement in collaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344efa4e-50ba-4d38-9ba2-5995d415a0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': [0.9121999740600586], '1': [0.3382999897003174]}\n",
      "0/2000 4.8376688957214355 19284.990234375 tensor(0.0898, device='cuda:3')\n",
      "50/2000 11.996919631958008 2384.12548828125 tensor(0.1172, device='cuda:3')\n",
      "100/2000 11.817474365234375 1874.72900390625 tensor(0.1094, device='cuda:3')\n",
      "150/2000 11.823150634765625 2288.601318359375 tensor(0.1328, device='cuda:3')\n",
      "200/2000 11.126581192016602 1965.8114013671875 tensor(0.1172, device='cuda:3')\n",
      "250/2000 11.111028671264648 2168.708251953125 tensor(0.1055, device='cuda:3')\n",
      "300/2000 11.280364036560059 1418.328857421875 tensor(0.1172, device='cuda:3')\n",
      "350/2000 11.276748657226562 1460.3328857421875 tensor(0.0938, device='cuda:3')\n",
      "400/2000 11.042470932006836 1159.742919921875 tensor(0.1211, device='cuda:3')\n",
      "450/2000 11.089993476867676 1060.806396484375 tensor(0.1211, device='cuda:3')\n",
      "500/2000 11.117476463317871 1193.2574462890625 tensor(0.1055, device='cuda:3')\n",
      "550/2000 11.115407943725586 1303.047119140625 tensor(0.1172, device='cuda:3')\n",
      "600/2000 10.667488098144531 1333.8822021484375 tensor(0.0977, device='cuda:3')\n",
      "650/2000 10.330965995788574 2660.16259765625 tensor(0.1094, device='cuda:3')\n",
      "700/2000 11.1484375 1483.628662109375 tensor(0.1289, device='cuda:3')\n",
      "750/2000 11.065625190734863 1130.99072265625 tensor(0.1055, device='cuda:3')\n",
      "800/2000 10.85161018371582 889.6673583984375 tensor(0.1133, device='cuda:3')\n",
      "850/2000 10.958941459655762 989.723876953125 tensor(0.1094, device='cuda:3')\n",
      "900/2000 10.420060157775879 1571.6549072265625 tensor(0.1016, device='cuda:3')\n",
      "950/2000 11.224261283874512 1308.3214111328125 tensor(0.1055, device='cuda:3')\n",
      "1000/2000 10.602824211120605 1083.965576171875 tensor(0.1211, device='cuda:3')\n",
      "1050/2000 10.99105453491211 1456.9444580078125 tensor(0.0977, device='cuda:3')\n",
      "1100/2000 10.496347427368164 926.4192504882812 tensor(0.1406, device='cuda:3')\n",
      "1150/2000 10.894920349121094 1055.346923828125 tensor(0.0898, device='cuda:3')\n",
      "1200/2000 10.466460227966309 1520.0531005859375 tensor(0.1055, device='cuda:3')\n",
      "1250/2000 10.704261779785156 983.133056640625 tensor(0.1172, device='cuda:3')\n",
      "1300/2000 10.766925811767578 795.9664916992188 tensor(0.0977, device='cuda:3')\n",
      "1350/2000 10.590424537658691 892.4174194335938 tensor(0.0977, device='cuda:3')\n",
      "1400/2000 10.584208488464355 866.7615966796875 tensor(0.1172, device='cuda:3')\n",
      "1450/2000 11.241523742675781 1864.4166259765625 tensor(0.0898, device='cuda:3')\n",
      "1500/2000 10.533554077148438 1143.1878662109375 tensor(0.0977, device='cuda:3')\n",
      "1550/2000 10.997138977050781 1363.45947265625 tensor(0.1172, device='cuda:3')\n",
      "1600/2000 10.867633819580078 1296.1656494140625 tensor(0.1133, device='cuda:3')\n",
      "1650/2000 10.261468887329102 1911.84521484375 tensor(0.0820, device='cuda:3')\n",
      "1700/2000 10.594551086425781 939.0938720703125 tensor(0.1250, device='cuda:3')\n",
      "1750/2000 10.597238540649414 898.774169921875 tensor(0.1289, device='cuda:3')\n",
      "1800/2000 10.549222946166992 935.9632568359375 tensor(0.1328, device='cuda:3')\n",
      "1850/2000 10.350483894348145 1346.86328125 tensor(0.1172, device='cuda:3')\n",
      "1900/2000 10.489728927612305 839.59619140625 tensor(0.1211, device='cuda:3')\n",
      "1950/2000 10.938643455505371 1996.790283203125 tensor(0.1094, device='cuda:3')\n",
      "training over 0\n",
      "training over 1\n",
      "{'0': [0.9121999740600586, 0.9140999913215637], '1': [0.3382999897003174, 0.33719998598098755]}\n",
      "0/2000 6.743929386138916 19686.986328125 tensor(0.1016, device='cuda:3')\n",
      "50/2000 12.596736907958984 2897.76611328125 tensor(0.0938, device='cuda:3')\n",
      "100/2000 11.33176040649414 3536.08203125 tensor(0.0938, device='cuda:3')\n",
      "150/2000 12.812859535217285 2821.7744140625 tensor(0.0977, device='cuda:3')\n",
      "200/2000 12.08732795715332 1682.6666259765625 tensor(0.0859, device='cuda:3')\n",
      "250/2000 12.477547645568848 1766.3245849609375 tensor(0.0820, device='cuda:3')\n",
      "300/2000 11.432168006896973 2361.583984375 tensor(0.0938, device='cuda:3')\n",
      "350/2000 11.476228713989258 2302.914306640625 tensor(0.0742, device='cuda:3')\n",
      "400/2000 12.198712348937988 1720.43798828125 tensor(0.0742, device='cuda:3')\n",
      "450/2000 11.998964309692383 1830.979736328125 tensor(0.1016, device='cuda:3')\n",
      "500/2000 12.426826477050781 2970.2099609375 tensor(0.0977, device='cuda:3')\n",
      "550/2000 11.287302017211914 1535.4700927734375 tensor(0.0977, device='cuda:3')\n",
      "600/2000 11.451384544372559 1086.5611572265625 tensor(0.0820, device='cuda:3')\n",
      "650/2000 11.782705307006836 2024.95703125 tensor(0.0977, device='cuda:3')\n",
      "700/2000 10.930495262145996 1341.7252197265625 tensor(0.0898, device='cuda:3')\n",
      "750/2000 11.37148666381836 1053.8978271484375 tensor(0.0898, device='cuda:3')\n",
      "800/2000 11.916969299316406 1675.8289794921875 tensor(0.0898, device='cuda:3')\n",
      "850/2000 11.258711814880371 1050.82763671875 tensor(0.0977, device='cuda:3')\n",
      "900/2000 11.546408653259277 1175.0572509765625 tensor(0.0898, device='cuda:3')\n",
      "950/2000 11.172085762023926 1170.0706787109375 tensor(0.0820, device='cuda:3')\n",
      "1000/2000 11.507410049438477 1579.770263671875 tensor(0.1133, device='cuda:3')\n",
      "1050/2000 10.719942092895508 1556.9088134765625 tensor(0.1094, device='cuda:3')\n",
      "1100/2000 10.9827241897583 1007.8798828125 tensor(0.1016, device='cuda:3')\n",
      "1150/2000 11.189323425292969 1025.3687744140625 tensor(0.0938, device='cuda:3')\n",
      "1200/2000 11.400511741638184 1318.595947265625 tensor(0.1133, device='cuda:3')\n",
      "1250/2000 11.28832721710205 1052.7083740234375 tensor(0.0938, device='cuda:3')\n",
      "1300/2000 10.724937438964844 1229.7281494140625 tensor(0.1055, device='cuda:3')\n",
      "1350/2000 10.860541343688965 1596.156494140625 tensor(0.1016, device='cuda:3')\n",
      "1400/2000 11.318880081176758 1440.239501953125 tensor(0.0859, device='cuda:3')\n",
      "1450/2000 11.049964904785156 915.26708984375 tensor(0.1055, device='cuda:3')\n",
      "1500/2000 10.687414169311523 892.6026611328125 tensor(0.1016, device='cuda:3')\n",
      "1550/2000 10.578978538513184 1534.8250732421875 tensor(0.1016, device='cuda:3')\n",
      "1600/2000 10.46965217590332 1629.0208740234375 tensor(0.1016, device='cuda:3')\n",
      "1650/2000 11.11911392211914 1210.985107421875 tensor(0.1016, device='cuda:3')\n",
      "1700/2000 10.91456413269043 1110.67333984375 tensor(0.1133, device='cuda:3')\n",
      "1750/2000 10.552849769592285 1009.3292236328125 tensor(0.0781, device='cuda:3')\n",
      "1800/2000 10.516359329223633 984.9921264648438 tensor(0.1055, device='cuda:3')\n",
      "1850/2000 10.903054237365723 855.91552734375 tensor(0.0977, device='cuda:3')\n",
      "1900/2000 10.786341667175293 962.9851684570312 tensor(0.0977, device='cuda:3')\n",
      "1950/2000 10.835028648376465 1013.794189453125 tensor(0.1055, device='cuda:3')\n",
      "training over 0\n",
      "training over 1\n",
      "{'0': [0.9121999740600586, 0.9140999913215637, 0.9049999713897705], '1': [0.3382999897003174, 0.33719998598098755, 0.2930999994277954]}\n",
      "0/2000 6.661704063415527 20777.89453125 tensor(0.1094, device='cuda:3')\n",
      "50/2000 11.951883316040039 3975.673095703125 tensor(0.1367, device='cuda:3')\n",
      "100/2000 11.17058277130127 2078.78271484375 tensor(0.1328, device='cuda:3')\n",
      "150/2000 11.61779499053955 2985.7333984375 tensor(0.1367, device='cuda:3')\n",
      "200/2000 10.953014373779297 1931.56494140625 tensor(0.1484, device='cuda:3')\n",
      "250/2000 10.446403503417969 2160.10205078125 tensor(0.1641, device='cuda:3')\n",
      "300/2000 10.647431373596191 1882.801025390625 tensor(0.1250, device='cuda:3')\n",
      "350/2000 10.923211097717285 1386.1103515625 tensor(0.1406, device='cuda:3')\n",
      "400/2000 10.850936889648438 1439.384765625 tensor(0.1289, device='cuda:3')\n",
      "450/2000 9.990997314453125 2166.236328125 tensor(0.1719, device='cuda:3')\n",
      "500/2000 10.703619956970215 1210.61328125 tensor(0.1367, device='cuda:3')\n",
      "550/2000 11.573525428771973 3086.01025390625 tensor(0.1523, device='cuda:3')\n",
      "600/2000 10.862470626831055 1930.495849609375 tensor(0.1445, device='cuda:3')\n",
      "650/2000 10.387046813964844 1255.4737548828125 tensor(0.1523, device='cuda:3')\n",
      "700/2000 10.70475959777832 1260.2760009765625 tensor(0.1445, device='cuda:3')\n",
      "750/2000 10.26276969909668 1378.942626953125 tensor(0.1562, device='cuda:3')\n",
      "800/2000 10.715095520019531 1799.5355224609375 tensor(0.1562, device='cuda:3')\n",
      "850/2000 10.452088356018066 1200.294189453125 tensor(0.1719, device='cuda:3')\n",
      "900/2000 10.9121675491333 1931.1251220703125 tensor(0.1523, device='cuda:3')\n",
      "950/2000 10.506587982177734 1436.1634521484375 tensor(0.1719, device='cuda:3')\n",
      "1000/2000 10.1826753616333 1125.0108642578125 tensor(0.1523, device='cuda:3')\n",
      "1050/2000 10.160662651062012 1096.3299560546875 tensor(0.1602, device='cuda:3')\n",
      "1100/2000 10.11884880065918 1635.3359375 tensor(0.1484, device='cuda:3')\n",
      "1150/2000 10.398366928100586 1074.5625 tensor(0.1250, device='cuda:3')\n",
      "1200/2000 10.795133590698242 1796.72802734375 tensor(0.1445, device='cuda:3')\n",
      "1250/2000 10.183069229125977 1687.7442626953125 tensor(0.1328, device='cuda:3')\n",
      "1300/2000 10.476860046386719 1114.5430908203125 tensor(0.1523, device='cuda:3')\n",
      "1350/2000 10.344481468200684 1045.961669921875 tensor(0.1445, device='cuda:3')\n",
      "1400/2000 10.357108116149902 1219.9815673828125 tensor(0.1367, device='cuda:3')\n",
      "1450/2000 10.289868354797363 988.7098999023438 tensor(0.1602, device='cuda:3')\n",
      "1500/2000 10.116434097290039 946.0755004882812 tensor(0.1680, device='cuda:3')\n",
      "1550/2000 10.274666786193848 1052.3770751953125 tensor(0.1641, device='cuda:3')\n",
      "1600/2000 10.042675971984863 1075.161865234375 tensor(0.1562, device='cuda:3')\n",
      "1650/2000 10.186779975891113 953.0905151367188 tensor(0.1602, device='cuda:3')\n",
      "1700/2000 10.284567832946777 1222.2144775390625 tensor(0.1562, device='cuda:3')\n",
      "1750/2000 10.44443130493164 1420.2159423828125 tensor(0.1484, device='cuda:3')\n",
      "1800/2000 9.979912757873535 1051.50390625 tensor(0.1367, device='cuda:3')\n",
      "1850/2000 9.956437110900879 1386.4625244140625 tensor(0.1562, device='cuda:3')\n",
      "1900/2000 9.99915599822998 966.5867309570312 tensor(0.1680, device='cuda:3')\n",
      "1950/2000 9.582085609436035 1975.75048828125 tensor(0.1641, device='cuda:3')\n",
      "training over 0\n",
      "training over 1\n",
      "{'0': [0.9121999740600586, 0.9140999913215637, 0.9049999713897705, 0.913599967956543], '1': [0.3382999897003174, 0.33719998598098755, 0.2930999994277954, 0.3528999984264374]}\n",
      "0/2000 7.9406585693359375 21320.28125 tensor(0.1016, device='cuda:3')\n",
      "50/2000 12.228713035583496 3167.204345703125 tensor(0.0938, device='cuda:3')\n",
      "100/2000 12.408193588256836 3208.041259765625 tensor(0.0977, device='cuda:3')\n",
      "150/2000 11.531989097595215 1725.4549560546875 tensor(0.1289, device='cuda:3')\n",
      "200/2000 11.646528244018555 1582.162841796875 tensor(0.1055, device='cuda:3')\n",
      "250/2000 11.36490249633789 1573.042236328125 tensor(0.1367, device='cuda:3')\n",
      "300/2000 11.041686058044434 2024.727783203125 tensor(0.1367, device='cuda:3')\n",
      "350/2000 11.504266738891602 1685.748291015625 tensor(0.1328, device='cuda:3')\n",
      "400/2000 10.2406005859375 3331.29052734375 tensor(0.1328, device='cuda:3')\n",
      "450/2000 11.046692848205566 1684.349365234375 tensor(0.1328, device='cuda:3')\n",
      "500/2000 11.37878704071045 1925.72216796875 tensor(0.1445, device='cuda:3')\n",
      "550/2000 10.676706314086914 1410.275146484375 tensor(0.1562, device='cuda:3')\n",
      "600/2000 11.175088882446289 2284.3759765625 tensor(0.1250, device='cuda:3')\n",
      "650/2000 10.913070678710938 1156.189208984375 tensor(0.1211, device='cuda:3')\n",
      "700/2000 11.67805290222168 2179.594970703125 tensor(0.1250, device='cuda:3')\n",
      "750/2000 11.037960052490234 1526.724853515625 tensor(0.1484, device='cuda:3')\n",
      "800/2000 10.458528518676758 1189.75 tensor(0.1523, device='cuda:3')\n",
      "850/2000 10.353886604309082 1438.61669921875 tensor(0.1289, device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "bs = 256\n",
    "position=0\n",
    "kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "fig, axs = plt.subplots(ncols=num_clients, nrows=1, figsize=(22, 3))\n",
    "\n",
    "for epoch in range(500):\n",
    "    collab_data = []\n",
    "    # Evaluate on the common test set\n",
    "    for client_num in range(num_clients):\n",
    "        c_model = c_models[client_num]\n",
    "        c_model.eval()\n",
    "        correct, total = 0, 0\n",
    "        for x, y_hat in test_loader:\n",
    "            with torch.no_grad():\n",
    "                x, y_hat = x.to(device), y_hat.to(device)\n",
    "                y = c_model(x, layer_num=0)\n",
    "                correct += (y.argmax(dim=1) == y_hat).sum()\n",
    "                total += x.shape[0]\n",
    "        acc = correct / total\n",
    "        new_acc_list[str(client_num)].append(acc.item())\n",
    "    print(new_acc_list)\n",
    "\n",
    "    # Generate collab data\n",
    "    for client_num in range(num_clients):\n",
    "        \"\"\" We generate a zero vector of n (num_classes dimension)\n",
    "        then we generate random numbers within range n and substitute\n",
    "        zero at every index obtained from random number to be 1\n",
    "        This way the zero vector becomes a random one-hot vector\n",
    "        \"\"\"\n",
    "        # zeroes = torch.zeros(bs, 10)\n",
    "        ind = torch.randint(low=0, high=10, size=(bs,))\n",
    "        # zeroes[torch.arange(start=0, end=bs), ind] = 1\n",
    "        # target = zeroes.to(device)\n",
    "        target = ind.to(device)\n",
    "        rand_imgs = torch.randn((bs, 3, 32, 32)).to(device)\n",
    "\n",
    "        c_model = c_models[client_num]\n",
    "        c_model.eval()\n",
    "        \n",
    "        updated_imgs = run_grad_ascent_on_data(rand_imgs, target, c_model, position=position,\n",
    "                                               img_model=None, alpha_ce=1., steps=2000).detach()\n",
    "        with torch.no_grad():\n",
    "            acts = c_model(updated_imgs, position).detach()\n",
    "        collab_data.append((updated_imgs, acts))\n",
    "        # TODO: Only take the first client seriously for now\n",
    "        break\n",
    "\n",
    "    # Train each client on their own data and collab data\n",
    "    for client_num in range(num_clients):\n",
    "        c_model = c_models[client_num]\n",
    "        c_model.train()\n",
    "        c_optim = c_optims[client_num]\n",
    "        c_dloader = c_dloaders[client_num]\n",
    "        # Train it 10 times on the same distilled dataset\n",
    "        for _ in range(10):\n",
    "            # if client_num == 1:\n",
    "            #     # we do not want client 1 to contribute for now because it wasn't trained on much data\n",
    "            #     break\n",
    "            for c_num, (x, y_hat) in enumerate(collab_data):\n",
    "                if c_num == client_num:\n",
    "                    # no need to train on its own distilled data\n",
    "                    continue\n",
    "                x, y_hat = x.to(device), y_hat.to(device)\n",
    "                c_optim.zero_grad()\n",
    "                y = c_model(x, position)\n",
    "                y = nn.functional.log_softmax(y, dim=1)\n",
    "                loss = kl_loss_fn(y, nn.functional.softmax(y_hat, dim=1))\n",
    "                loss.backward()\n",
    "                c_optim.step()\n",
    "        for x, y_hat in c_dloader:\n",
    "            x, y_hat = x.to(device), y_hat.to(device)\n",
    "            c_optim.zero_grad()\n",
    "            y = c_model(x, position)\n",
    "            loss = loss_fn(y, y_hat)\n",
    "            loss.backward()\n",
    "            c_optim.step()\n",
    "        print(f\"training over {client_num}\")\n",
    "    # del collab_data, updated_imgs, acts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6038bf72-6db1-48d9-bfee-c87b3ca0ed63",
   "metadata": {},
   "source": [
    "### Test for multi-gpu speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "a40ca028-1ec1-4a7c-baf0-45906d0e9570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_models[0].module.load_state_dict(torch.load(\"./expt_dump/iid_clients_adaptive_collab_cifar10_2clients_2000samples_1_distill_epochs_10_steps_2000_seed1/saved_models/c0.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6708e-3b6d-459d-a0ef-5ef6001dd44e",
   "metadata": {},
   "source": [
    "# Deep Image Prior on the random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "b124aafe-dff9-41e0-9059-3ee6b9ece7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generative_model import skip\n",
    "\n",
    "gen_model = skip(32, 3, num_channels_down = [16, 32, 64, 128],\n",
    "                                      num_channels_up =   [16, 32, 64, 128],\n",
    "                                      num_channels_skip = [0, 4, 4, 4],\n",
    "                                      filter_size_down = [7, 7, 5, 5],  # type: ignore\n",
    "                                      filter_size_up = [7, 7, 5, 5],   # type: ignore\n",
    "                      upsample_mode='bilinear', downsample_mode='avg', need_sigmoid=True, pad='zero', act_fun='LeakyReLU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c93c573d-1334-45f7-b3ce-c3d81d7f6344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 0, 0, 3, 5, 0, 3, 2, 1, 8, 8, 4, 4, 8, 5, 0, 0, 9, 6, 9, 6, 2, 6, 3,\n",
      "        5, 8, 0, 3, 0, 8, 3, 2, 3, 1, 8, 8, 8, 4, 8, 1, 2, 0, 0, 5, 3, 0, 4, 4,\n",
      "        8, 9, 7, 8, 8, 8, 3, 8, 9, 7, 0, 0, 0, 5, 3, 2, 0, 3, 7, 9, 6, 8, 7, 8,\n",
      "        2, 9, 5, 9, 9, 5, 0, 7, 8, 6, 9, 3, 1, 0, 8, 8, 8, 6, 8, 7, 4, 8, 3, 5,\n",
      "        9, 4, 0, 4, 6, 3, 4, 1, 3, 8, 5, 4, 9, 1, 4, 1, 8, 9, 1, 1, 2, 7, 1, 0,\n",
      "        2, 0, 1, 9, 7, 4, 3, 0, 6, 0, 2, 7, 6, 1, 8, 0, 7, 7, 4, 3, 7, 0, 3, 3,\n",
      "        0, 3, 4, 1, 8, 6, 7, 0, 5, 0, 2, 4, 2, 2, 9, 7, 9, 2, 8, 4, 5, 0, 8, 7,\n",
      "        7, 4, 3, 6, 2, 6, 2, 3, 1, 1, 3, 9, 8, 8, 3, 4, 1, 2, 6, 6, 1, 1, 3, 1,\n",
      "        4, 9, 3, 2, 5, 5, 5, 6, 0, 6, 5, 1, 1, 4, 6, 7, 1, 7, 2, 9, 0, 9, 4, 9,\n",
      "        1, 7, 1, 4, 6, 6, 5, 3, 9, 8, 5, 0, 1, 7, 7, 3, 6, 8, 8, 0, 7, 9, 0, 0,\n",
      "        2, 4, 4, 4, 6, 1, 3, 4, 6, 3, 1, 6, 5, 9, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "bs = 256\n",
    "rand_imgs = torch.randn((bs, 64, 32, 32))\n",
    "zeroes = torch.zeros(bs, 10)\n",
    "\n",
    "ind = torch.randint(low=0, high=10, size=(bs,))\n",
    "# ind *= 5\n",
    "# ind += 1\n",
    "# zeroes[torch.arange(start=0, end=bs), ind] = 1\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6763e73f-2a4d-4fbc-a689-4b8a0a2d2924",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeepInversionFeatureHook' object has no attribute 'r_feature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m updated_imgs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_grad_ascent_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrand_imgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mimg_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43malpha_ce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_l2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_tv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mrun_grad_ascent_on_data\u001b[0;34m(inp, target_label, model, position, img_model, alpha_ce, alpha_f, alpha_tv, alpha_l2, lr, steps)\u001b[0m\n\u001b[1;32m     37\u001b[0m ce_loss \u001b[38;5;241m=\u001b[39m loss_fn(acts, target_label)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# ce_loss = nn.MSELoss()(acts, target_label)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m loss_r_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([model\u001b[38;5;241m.\u001b[39mr_feature \u001b[38;5;28;01mfor\u001b[39;00m (idx, model) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loss_r_feature_layers)])\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m alpha_ce \u001b[38;5;241m*\u001b[39m ce_loss \u001b[38;5;241m+\u001b[39m alpha_tv \u001b[38;5;241m*\u001b[39m tv_loss(updated_img) \u001b[38;5;241m+\u001b[39m alpha_l2 \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(updated_img) \u001b[38;5;241m+\u001b[39m alpha_f \u001b[38;5;241m*\u001b[39m loss_r_feature\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m ce_loss \u001b[38;5;241m=\u001b[39m loss_fn(acts, target_label)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# ce_loss = nn.MSELoss()(acts, target_label)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m loss_r_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr_feature\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m (idx, model) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loss_r_feature_layers)])\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m alpha_ce \u001b[38;5;241m*\u001b[39m ce_loss \u001b[38;5;241m+\u001b[39m alpha_tv \u001b[38;5;241m*\u001b[39m tv_loss(updated_img) \u001b[38;5;241m+\u001b[39m alpha_l2 \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(updated_img) \u001b[38;5;241m+\u001b[39m alpha_f \u001b[38;5;241m*\u001b[39m loss_r_feature\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DeepInversionFeatureHook' object has no attribute 'r_feature'"
     ]
    }
   ],
   "source": [
    "updated_imgs = run_grad_ascent_on_data(rand_imgs.to(device), ind.to(device), c_models[0], position=1,\n",
    "                                       img_model=None, lr=0.1,\n",
    "                                       alpha_ce=1., alpha_f=10., alpha_l2=0., alpha_tv=0.001,\n",
    "                                       steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c832ed1-c58f-4320-85fd-aa2d26db16a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: the launch timed out and was terminated\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m axs[\u001b[38;5;241m1\u001b[39m][i]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# axs[2][i].axis(\"off\")\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m axs[\u001b[38;5;241m0\u001b[39m][i]\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39marray((\u001b[43mrand_imgs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)[:,:,:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu())\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# axs[1][i].imshow(np.array((img_prior[i].detach()*255).permute(1, 2, 0).cpu()).astype(np.uint8))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m axs[\u001b[38;5;241m1\u001b[39m][i]\u001b[38;5;241m.\u001b[39mimshow(deprocess(updated_imgs)[i]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: the launch timed out and was terminated\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x360 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(ncols=10, nrows=2, figsize=(17, 5))\n",
    "for i in range(10):\n",
    "    axs[0][i].axis(\"off\")\n",
    "    axs[1][i].axis(\"off\")\n",
    "    # axs[2][i].axis(\"off\")\n",
    "    axs[0][i].imshow(np.array((rand_imgs[i].detach()*255).permute(1, 2, 0)[:,:,:3].cpu()).astype(np.uint8))\n",
    "    # axs[1][i].imshow(np.array((img_prior[i].detach()*255).permute(1, 2, 0).cpu()).astype(np.uint8))\n",
    "    axs[1][i].imshow(deprocess(updated_imgs)[i].permute(1, 2, 0).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43a7efba-aee7-44dc-81fe-4ff2a6989316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import utils\n",
    "utils.save_image(updated_imgs[:20].clone(),\n",
    "                      './removeme1.png',\n",
    "                      normalize=True, scale_each = True, nrow=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f85d9f1b-14d9-499d-8368-221dbbc7e575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0067, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(abs(updated_imgs[0] - updated_imgs[1])).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ca6827a-fc0d-4c38-b767-a51e7aeaa46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize an image\n",
    "def deprocess(img):\n",
    "    inv_normalize = T.Normalize(\n",
    "       mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "       std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "    img = inv_normalize(img)\n",
    "    img = 255*img\n",
    "    return img.type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1d1fbe2b-f72c-4b51-9f73-d51319ac673a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.2234,  -6.3234, -26.0120, -25.0174,  -3.9302,  -6.2938, -26.3084,\n",
       "         -26.1992, -25.7642, -25.9508],\n",
       "        [ -4.4034,  -3.2350, -21.4144, -20.0738,  -4.0045,  -4.0403, -19.8059,\n",
       "         -20.7658, -21.2401, -20.8493],\n",
       "        [ -5.8798,  -4.2963, -27.7720, -25.6363,  -6.0319,  -4.6734, -25.3708,\n",
       "         -26.5396, -26.8712, -26.6936],\n",
       "        [-12.5551,  -6.1844, -40.4543, -40.2059, -10.9165, -12.7064, -39.0421,\n",
       "         -39.0891, -40.4680, -40.0667],\n",
       "        [ -5.9261,  -3.7569, -27.0156, -25.1447,  -5.3495,  -5.1784, -24.8002,\n",
       "         -25.9212, -26.3600, -25.9734],\n",
       "        [ -8.8144, -13.4271, -34.4055, -33.2453,  -3.5201,  -9.0524, -35.1161,\n",
       "         -34.4815, -34.2569, -34.5227],\n",
       "        [ -4.7298,  -3.2915, -20.1200, -18.4321,  -2.6959,  -4.0940, -18.3464,\n",
       "         -19.4940, -19.7146, -19.5202],\n",
       "        [ -5.9741,  -4.1775, -23.3758, -22.1293,  -3.1037,  -5.5536, -23.1064,\n",
       "         -23.3743, -23.2530, -23.2161],\n",
       "        [ -6.0474,  -6.0017, -24.0202, -22.8700,  -3.0065,  -5.5183, -24.1800,\n",
       "         -24.1969, -23.8670, -23.8652],\n",
       "        [-15.4198,  -6.9618, -46.9595, -46.3724, -12.0369, -14.6843, -45.0301,\n",
       "         -44.9272, -46.9288, -46.3884],\n",
       "        [ -6.7908,  -3.4842, -32.0658, -30.0240,  -8.1827,  -6.1200, -29.2443,\n",
       "         -30.9770, -31.3511, -31.1071],\n",
       "        [ -6.3940,  -9.7479, -26.6370, -25.5055,  -2.5134,  -6.4078, -27.2623,\n",
       "         -26.9522, -26.5406, -26.6474],\n",
       "        [ -5.4535,  -4.1722, -22.4295, -21.4501,  -3.3653,  -5.0509, -22.3715,\n",
       "         -22.4211, -22.2684, -22.4932],\n",
       "        [-15.6462,  -7.0233, -48.2513, -47.7982, -12.7393, -14.9889, -46.2572,\n",
       "         -46.2998, -48.2796, -47.6545],\n",
       "        [ -6.3546,  -3.7761, -30.0827, -28.0116,  -6.7106,  -5.8024, -27.4672,\n",
       "         -28.9866, -29.2851, -29.0846],\n",
       "        [ -4.7962,  -3.4069, -21.1129, -19.3467,  -3.1513,  -4.0209, -19.3104,\n",
       "         -20.3751, -20.6796, -20.3846],\n",
       "        [ -6.5705,  -9.8214, -27.1409, -26.0322,  -2.8096,  -6.6103, -27.5887,\n",
       "         -27.4601, -26.9854, -26.9411],\n",
       "        [ -5.7310,  -5.7866, -23.2627, -22.1843,  -3.0062,  -5.0578, -23.2549,\n",
       "         -23.4772, -23.1630, -23.1064],\n",
       "        [ -3.2978,  -4.4981, -22.0199, -20.7499,  -5.5850,  -3.0847, -20.5783,\n",
       "         -21.4257, -21.6465, -21.3246],\n",
       "        [ -3.4268,  -4.5521, -21.3637, -19.8120,  -4.9403,  -2.7278, -19.7638,\n",
       "         -20.5639, -20.7569, -20.6907],\n",
       "        [ -2.8232,  -4.0316, -17.7736, -16.6820,  -4.3134,  -2.4135, -16.9555,\n",
       "         -17.2182, -17.6280, -17.4878],\n",
       "        [-13.0285,  -6.6657, -41.6676, -41.6145, -11.6534, -12.7560, -40.4642,\n",
       "         -40.3991, -41.8785, -41.5090],\n",
       "        [ -3.2841,  -4.3366, -20.2728, -18.9165,  -4.7965,  -2.5513, -19.1100,\n",
       "         -19.6322, -19.9537, -19.7860],\n",
       "        [-11.1722, -14.0116, -38.7353, -37.2514,  -2.7790, -11.6314, -39.5157,\n",
       "         -38.7595, -38.5464, -38.7504]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_models[0](rand_imgs)[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bde2e226-271d-4dd0-b650-3bbbce9a3fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroes.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e357694-43d0-413b-b3c5-fdd1e97814f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 0, 0,  ..., 0, 0, 0], device='cuda:0') tensor([3, 8, 8,  ..., 1, 0, 0], device='cuda:0')\n",
      "tensor([5, 5, 0,  ..., 0, 5, 5], device='cuda:0') tensor([3, 5, 0,  ..., 1, 3, 5], device='cuda:0')\n",
      "tensor([0, 5, 0,  ..., 0, 0, 5], device='cuda:0') tensor([8, 3, 8,  ..., 4, 0, 7], device='cuda:0')\n",
      "tensor([0, 5, 0,  ..., 5, 5, 0], device='cuda:0') tensor([8, 3, 1,  ..., 3, 3, 8], device='cuda:0')\n",
      "tensor([0, 5, 0,  ..., 5, 0, 5], device='cuda:0') tensor([8, 3, 9,  ..., 7, 2, 5], device='cuda:0')\n",
      "tensor([0, 5, 5,  ..., 0, 5, 5], device='cuda:0') tensor([9, 6, 6,  ..., 8, 5, 9], device='cuda:0')\n",
      "tensor([5, 5, 0,  ..., 0, 5, 5], device='cuda:0') tensor([7, 2, 0,  ..., 0, 1, 7], device='cuda:0')\n",
      "tensor([5, 0, 0,  ..., 0, 5, 5], device='cuda:0') tensor([5, 1, 9,  ..., 8, 4, 6], device='cuda:0')\n",
      "tensor([5, 5, 0,  ..., 5, 5, 5], device='cuda:0') tensor([5, 3, 1,  ..., 2, 9, 5], device='cuda:0')\n",
      "tensor([5, 0, 5, 5, 0, 5, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 0, 5, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5,\n",
      "        5, 0, 0, 5, 5, 5, 0, 0, 0, 0, 0, 0, 5, 5, 5, 0, 0, 0, 5, 0, 5, 0, 0, 0,\n",
      "        5, 5, 5, 0, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5,\n",
      "        0, 5, 0, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 0, 0, 5, 0,\n",
      "        5, 5, 0, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 0, 5, 0, 0,\n",
      "        0, 0, 5, 5, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 0,\n",
      "        0, 5, 5, 5, 5, 0, 5, 0, 0, 5, 5, 0, 0, 0, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5,\n",
      "        0, 5, 5, 5, 0, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 0, 5, 0, 5, 0, 5, 5, 5,\n",
      "        0, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 0, 0, 5, 5, 5, 0, 5, 0, 0, 5, 5, 0,\n",
      "        5, 0, 5, 0, 5, 5, 0, 0, 5, 5, 0, 5, 5, 0, 0, 5, 0, 0, 0, 5, 0, 5, 0, 0,\n",
      "        5, 5, 0, 0, 0, 5, 0, 0, 5, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 5, 5,\n",
      "        5, 5, 0, 0, 0, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 5,\n",
      "        0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 0, 5, 0, 0, 5, 0, 0, 5, 5, 0, 5, 5, 0,\n",
      "        0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 5, 5, 5, 0, 0, 5, 0, 5, 5, 0, 0, 5, 5, 5,\n",
      "        5, 0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 0, 0, 0, 5, 0, 5, 5, 5, 5, 5,\n",
      "        0, 5, 0, 5, 0, 5, 0, 0, 0, 0, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 0,\n",
      "        0, 0, 0, 5, 0, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 5, 5,\n",
      "        5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 5, 5, 5, 5, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5,\n",
      "        5, 0, 0, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0,\n",
      "        0, 0, 5, 0, 0, 5, 5, 0, 5, 5, 5, 5, 0, 0, 5, 5, 5, 0, 0, 5, 5, 0, 0, 0,\n",
      "        0, 5, 5, 5, 0, 5, 0, 0, 5, 5, 5, 5, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0,\n",
      "        0, 5, 0, 0, 5, 0, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 0, 0,\n",
      "        5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 5, 0, 5,\n",
      "        0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 5, 5, 0, 0, 5,\n",
      "        0, 5, 0, 5, 0, 5, 0, 5, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 5, 5,\n",
      "        5, 0, 5, 0, 5, 5, 0, 0, 0, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5,\n",
      "        0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 5, 0, 0, 0, 5, 5, 5, 5, 0, 5, 0, 0, 5, 5,\n",
      "        0, 0, 0, 5, 5, 5, 0, 5, 5, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0,\n",
      "        0, 5, 0, 0, 0, 5, 5, 5, 5, 0, 5, 5, 0, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 5,\n",
      "        0, 5, 0, 5, 5, 0, 0, 0, 5, 5, 0, 5, 0, 0, 5, 0, 5, 5, 0, 0, 0, 0, 0, 5,\n",
      "        0, 5, 5, 0, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 0, 0, 5, 5, 5, 5, 0, 5, 5, 0,\n",
      "        5, 5, 0, 0, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 5], device='cuda:0') tensor([6, 9, 3, 9, 8, 7, 7, 1, 6, 5, 3, 1, 3, 1, 2, 7, 1, 8, 2, 0, 9, 7, 9, 8,\n",
      "        8, 6, 7, 3, 7, 1, 3, 9, 0, 9, 3, 6, 7, 2, 7, 3, 0, 5, 9, 7, 5, 5, 0, 6,\n",
      "        5, 1, 8, 2, 7, 5, 9, 0, 0, 0, 8, 8, 7, 3, 7, 8, 9, 3, 7, 9, 7, 8, 7, 9,\n",
      "        8, 5, 4, 8, 3, 7, 6, 3, 8, 2, 1, 9, 5, 7, 3, 9, 5, 5, 8, 7, 3, 5, 3, 5,\n",
      "        9, 7, 6, 7, 3, 6, 4, 3, 9, 4, 2, 1, 9, 6, 0, 2, 6, 7, 4, 7, 9, 0, 7, 4,\n",
      "        3, 5, 3, 1, 1, 2, 6, 8, 2, 1, 7, 8, 5, 9, 6, 1, 1, 5, 0, 6, 0, 9, 2, 6,\n",
      "        5, 8, 9, 5, 5, 6, 2, 9, 1, 5, 8, 8, 7, 1, 7, 3, 5, 4, 9, 7, 5, 2, 9, 9,\n",
      "        4, 7, 4, 1, 3, 8, 7, 9, 0, 4, 5, 7, 5, 2, 8, 7, 6, 9, 6, 9, 3, 8, 5, 6,\n",
      "        6, 9, 5, 7, 8, 0, 5, 0, 7, 4, 8, 2, 5, 1, 3, 2, 2, 6, 2, 1, 7, 4, 6, 3,\n",
      "        1, 3, 7, 2, 1, 3, 7, 0, 8, 4, 4, 5, 7, 9, 5, 4, 3, 9, 6, 8, 2, 3, 3, 1,\n",
      "        6, 1, 7, 0, 3, 4, 2, 9, 4, 5, 8, 2, 7, 0, 9, 6, 8, 0, 8, 2, 8, 5, 7, 7,\n",
      "        2, 2, 0, 0, 0, 7, 4, 1, 6, 6, 8, 8, 9, 0, 9, 0, 1, 3, 3, 0, 9, 6, 6, 2,\n",
      "        6, 3, 4, 0, 8, 4, 1, 4, 0, 6, 5, 0, 9, 9, 9, 9, 1, 2, 3, 5, 4, 2, 9, 6,\n",
      "        0, 9, 6, 6, 8, 0, 6, 1, 4, 6, 8, 0, 5, 4, 1, 2, 0, 9, 6, 4, 2, 4, 6, 5,\n",
      "        9, 7, 7, 4, 6, 5, 0, 1, 9, 0, 3, 1, 9, 0, 9, 7, 8, 6, 7, 6, 8, 2, 4, 5,\n",
      "        3, 0, 3, 2, 1, 7, 5, 9, 3, 4, 5, 7, 1, 5, 0, 1, 1, 1, 9, 7, 5, 4, 9, 7,\n",
      "        8, 1, 0, 2, 8, 5, 6, 7, 0, 1, 4, 8, 4, 4, 6, 6, 5, 8, 1, 8, 4, 6, 5, 9,\n",
      "        2, 2, 1, 4, 9, 1, 6, 7, 2, 0, 1, 7, 6, 5, 2, 2, 5, 6, 0, 9, 0, 1, 5, 3,\n",
      "        3, 5, 8, 7, 5, 6, 5, 8, 0, 5, 9, 4, 6, 5, 1, 1, 0, 3, 3, 9, 4, 8, 1, 7,\n",
      "        7, 9, 9, 4, 3, 6, 3, 2, 8, 2, 7, 6, 7, 0, 2, 1, 2, 9, 4, 6, 9, 6, 1, 0,\n",
      "        1, 8, 7, 0, 0, 4, 7, 4, 2, 6, 9, 5, 9, 0, 7, 4, 5, 8, 1, 4, 7, 9, 9, 8,\n",
      "        8, 6, 3, 7, 0, 8, 9, 6, 2, 4, 6, 2, 9, 7, 4, 6, 8, 5, 6, 1, 3, 5, 9, 9,\n",
      "        1, 3, 2, 0, 3, 0, 2, 0, 7, 3, 9, 3, 5, 7, 6, 5, 9, 5, 6, 1, 2, 4, 0, 2,\n",
      "        7, 8, 4, 4, 0, 9, 5, 9, 2, 9, 3, 2, 4, 3, 2, 2, 8, 8, 6, 8, 1, 6, 8, 9,\n",
      "        8, 2, 1, 4, 9, 1, 5, 7, 1, 6, 0, 1, 5, 2, 8, 1, 1, 3, 8, 1, 3, 1, 8, 5,\n",
      "        1, 9, 0, 3, 0, 4, 0, 5, 1, 2, 0, 4, 9, 5, 1, 2, 2, 7, 8, 7, 2, 4, 5, 3,\n",
      "        4, 0, 6, 1, 5, 9, 8, 0, 2, 0, 7, 6, 0, 5, 9, 7, 5, 0, 4, 6, 0, 6, 3, 6,\n",
      "        8, 1, 1, 8, 9, 7, 2, 9, 0, 2, 2, 9, 0, 6, 5, 7, 7, 9, 1, 7, 9, 8, 4, 5,\n",
      "        0, 8, 0, 2, 5, 2, 1, 4, 4, 8, 9, 7, 8, 3, 6, 6, 0, 1, 1, 1, 8, 1, 4, 4,\n",
      "        0, 7, 8, 2, 1, 2, 5, 4, 6, 0, 5, 7, 4, 4, 3, 9, 5, 8, 8, 0, 8, 7, 4, 1,\n",
      "        8, 4, 9, 5, 4, 1, 7, 7, 7, 7, 0, 3, 8, 3, 3, 0, 5, 7, 0, 8, 0, 0, 9, 2,\n",
      "        2, 3, 4, 8, 2, 2, 6, 3, 3, 6, 2, 9, 4, 0, 1, 7, 5, 5, 7, 3, 0, 4, 2, 0,\n",
      "        7, 5, 8, 0, 8, 2, 7, 0, 3, 5, 3, 8, 3, 5, 1, 7], device='cuda:0')\n",
      "client 1, acc 0.1912\n"
     ]
    }
   ],
   "source": [
    "c_models[0].eval()\n",
    "correct, total = 0, 0\n",
    "for x, y_hat in test_loader:\n",
    "    with torch.no_grad():\n",
    "        x, y_hat = x.to(device), y_hat.to(device)\n",
    "        y = c_models[0](x)\n",
    "        correct += (y.argmax(dim=1) == y_hat).sum()\n",
    "        total += x.shape[0]\n",
    "        print(y.argmax(dim=1), y_hat)\n",
    "print(\"client {}, acc {:.4f}\".format(client_num, correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9dabb-2618-4346-96e5-4c38c2709a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
