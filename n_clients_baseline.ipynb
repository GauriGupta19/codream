{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50732264-d0ba-4e3a-bfe8-b274d9cdf1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "import PIL.Image\n",
    "import torch.cuda.amp as amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c22f5fc-0414-4b05-81b7-0e4a1d49707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.cifar import CIFAR10\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a154bc4-247b-4088-8521-0d35858c2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afe805d-b383-4dbe-94ad-4f0698f4e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_ids = [0, 1, 2, 3]\n",
    "batch_size = len(device_ids) * 256\n",
    "device = torch.device('cuda:{}'.format(device_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a631b310-6ce1-483c-8468-e7f80d4438d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 32\n",
    "num_classes = 10\n",
    "\n",
    "train_transform = T.Compose(\n",
    "    [\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), \n",
    "            (0.2023, 0.1994, 0.2010)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "test_transform = T.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), \n",
    "            (0.2023, 0.1994, 0.2010)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_dataset = CIFAR10(\n",
    "    root=\"./imgs/cifar10\", train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_dataset = CIFAR10(\n",
    "    root=\"./imgs/cifar10\", train=False, download=True, transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884e5291-9781-4f4d-be0b-d484f94c4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21cc3e34-9962-4f2f-ae12-92e386398c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6daaf69e-ec62-4e80-84d6-95e08d663baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam\n",
    "\n",
    "c_models = []\n",
    "c_optims = []\n",
    "c_dsets = []\n",
    "c_dloaders = []\n",
    "\n",
    "num_clients = 2\n",
    "samples_per_client = 10000\n",
    "\n",
    "for i in range(num_clients):\n",
    "    c_model = nn.DataParallel(models.resnet34().to(device), device_ids=device_ids)\n",
    "    c_optim = optim(c_model.parameters(), lr=3e-4)\n",
    "    c_idx = indices[i*samples_per_client: (i+1)*samples_per_client]\n",
    "    c_dset = Subset(train_dataset, c_idx)\n",
    "    c_dloader = DataLoader(c_dset, batch_size=64*len(device_ids), shuffle=True)\n",
    "\n",
    "    c_models.append(c_model)\n",
    "    c_optims.append(c_optim)\n",
    "    c_dsets.append(c_dset)\n",
    "    c_dloaders.append(c_dloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63dfc7-04b6-4b30-a8e2-83b40bcedb49",
   "metadata": {},
   "source": [
    "## Isolated Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "166a3919-1d17-4d32-ae69-a49ad6ad03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = {}\n",
    "for i in range(num_clients):\n",
    "    acc_list[str(i)] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "587fb34c-c573-4f64-8436-9287f7ef5b2f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1186, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torchvision/models/resnet.py\", line 285, in forward\n    return self._forward_impl(x)\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torchvision/models/resnet.py\", line 278, in _forward_impl\n    x = self.avgpool(x)\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1186, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/modules/pooling.py\", line 1183, in forward\n    return F.adaptive_avg_pool2d(input, self.output_size)\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/functional.py\", line 1214, in adaptive_avg_pool2d\n    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)\nRuntimeError: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m     x, y_hat \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y_hat\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 12\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mc_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (y\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y_hat)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     14\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:170\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    169\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 170\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:180\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1186, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torchvision/models/resnet.py\", line 285, in forward\n    return self._forward_impl(x)\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torchvision/models/resnet.py\", line 278, in _forward_impl\n    x = self.avgpool(x)\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1186, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/modules/pooling.py\", line 1183, in forward\n    return F.adaptive_avg_pool2d(input, self.output_size)\n  File \"/u/abhi24/matlaberp3/.torchP3/lib/python3.10/site-packages/torch/nn/functional.py\", line 1214, in adaptive_avg_pool2d\n    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)\nRuntimeError: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1584x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_epochs = 200\n",
    "fig, axs = plt.subplots(ncols=num_clients, nrows=1, figsize=(22, 3))\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    for client_num in range(num_clients):\n",
    "        c_model = c_models[client_num]\n",
    "        c_model.eval()\n",
    "        correct, total = 0, 0\n",
    "        for x, y_hat in test_loader:\n",
    "            with torch.no_grad():\n",
    "                x, y_hat = x.to(device), y_hat.to(device)\n",
    "                y = c_model(x)\n",
    "                correct += (y.argmax(dim=1) == y_hat).sum()\n",
    "                total += x.shape[0]\n",
    "        acc = correct / total\n",
    "        # print(\"epoch {}, client {}, acc {:.4f}\".format(epoch, client_num, acc))\n",
    "        acc_list[str(client_num)].append(acc.item())\n",
    "    for client_num in range(num_clients):\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        axs[client_num].plot(acc_list[str(client_num)])\n",
    "\n",
    "    for client_num in range(num_clients):\n",
    "        c_model = c_models[client_num]\n",
    "        c_model.train()\n",
    "        c_optim = c_optims[client_num]\n",
    "        c_dloader = c_dloaders[client_num]\n",
    "        for x, y_hat in c_dloader:\n",
    "            x, y_hat = x.to(device), y_hat.to(device)\n",
    "            c_optim.zero_grad()\n",
    "            y = c_model(x)\n",
    "            loss = loss_fn(y, y_hat)\n",
    "            loss.backward()\n",
    "            c_optim.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f5c00-84d2-4799-869b-1f6b1eca4cc0",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f482a-07df-4256-b60c-3c8c090b3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clients):\n",
    "    torch.save(c_models[i].module.state_dict(), f\"./saved_models/c{i}_isolated.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4519fe-b2ef-420a-95aa-691063102205",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2aa387-71ba-405f-bcb2-712cbef101dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clients):\n",
    "    c_models[i].module.load_state_dict(torch.load(f\"./saved_models/c{i}_isolated.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "053cc73e-4e21-4d1d-8a6a-3c79921b3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clients):\n",
    "    c_models[i].module.load_state_dict(torch.load(f\"./saved_models/c{i}_collab.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551de768-9b44-4335-871a-9f6cdad38ad0",
   "metadata": {},
   "source": [
    "## Evaluate on a common test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a7c7671-ee3e-4a88-ba8a-41359b9e1f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client 0, acc 0.0000\n",
      "client 1, acc 0.0008\n"
     ]
    }
   ],
   "source": [
    "for client_num in range(num_clients):\n",
    "    c_model = c_models[client_num]\n",
    "    c_model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x, y_hat in test_loader:\n",
    "        with torch.no_grad():\n",
    "            x, y_hat = x.to(device), y_hat.to(device)\n",
    "            y = c_model(x)\n",
    "            correct += (y.argmax(dim=1) == y_hat).sum()\n",
    "            total += x.shape[0]\n",
    "    print(\"client {}, acc {:.4f}\".format(client_num, correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01482ac-510a-4d73-8acb-2b4500b8a0d6",
   "metadata": {},
   "source": [
    "## Collaborative Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4951d95-8814-4613-841d-5e7d156c8fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import DeepInversionFeatureHook, total_variation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cf4bb12-0a05-4c78-8631-1bee6532c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=np.array((0.4914, 0.4822, 0.4465))\n",
    "std=np.array((0.2023, 0.1994, 0.2010))\n",
    "\n",
    "LOWER_IMAGE_BOUND = torch.tensor((-mean / std).reshape(1, -1, 1, 1)).float().to(device)\n",
    "UPPER_IMAGE_BOUND = torch.tensor(((1 - mean) / std).reshape(1, -1, 1, 1)).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e866ca53-774c-4eb1-b6f5-480e26e98b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1bfb72e-5ac3-4a19-b2b0-c46241b9f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grad_ascent_on_data(orig_img, target_label, model, lr=0.05, steps=50):\n",
    "    start = time.time()\n",
    "    # scaler = GradScaler()\n",
    "    # print(\"BEFORE: memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    # print(\"BEFORE: memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"BEFORE: max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "    alpha_tv, alpha_l2, alpha_f = 2.5e-5, 3e-8, 10.0\n",
    "    loss_r_feature_layers = []\n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            loss_r_feature_layers.append(DeepInversionFeatureHook(module))\n",
    "\n",
    "    updated_img = orig_img.detach()#.clone()\n",
    "    updated_img.requires_grad = True\n",
    "    updated_img.retain_grad()\n",
    "    print(\"MIDDLE: max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    lim_0, lim_1 = 2, 2\n",
    "    for it in range(steps):\n",
    "        off1 = random.randint(-lim_0, lim_0)\n",
    "        off2 = random.randint(-lim_1, lim_1)\n",
    "        inputs_jit = torch.roll(updated_img.detach(), shifts=(off1, off2), dims=(2,3))\n",
    "\n",
    "        # with autocast(device_type='cuda', dtype=torch.float16):\n",
    "        acts = model.module(inputs_jit)[:, :10]\n",
    "        # ce_loss = nn.functional.mse_loss(target_label, acts).sum()\n",
    "        ce_loss = nn.CrossEntropyLoss()(acts, target_label)\n",
    "\n",
    "        # rescale = [first_bn_multiplier] + [1. for _ in range(len(loss_r_feature_layers)-1)]\n",
    "        loss_r_feature = sum([model.r_feature for (idx, model) in enumerate(loss_r_feature_layers)])\n",
    "        loss = ce_loss + alpha_tv * total_variation_loss(updated_img) + alpha_l2 * torch.linalg.norm(updated_img) + alpha_f * loss_r_feature\n",
    "\n",
    "        loss.backward()\n",
    "        # scaler.scale(loss).backward()\n",
    "        # if it % 10 == 0:\n",
    "        #     display.clear_output(wait=True)\n",
    "        #     show(deprocess(updated_img[0]).detach().permute(1,2,0))\n",
    "        #     print((updated_img - orig_img).mean())\n",
    "        grads = updated_img.grad.data / (torch.std(updated_img.grad.data) + 1e-8)\n",
    "        updated_img.data = updated_img.data - lr * grads\n",
    "        updated_img.data = torch.clamp(updated_img.data, min=LOWER_IMAGE_BOUND, max=UPPER_IMAGE_BOUND)\n",
    "        # optimizer.step()\n",
    "        model.zero_grad()\n",
    "        # optimizer.zero_grad()\n",
    "        updated_img.grad.data.zero_()\n",
    "    for item in loss_r_feature_layers:\n",
    "        item.close()\n",
    "    k = updated_img.detach().clone()\n",
    "    print(\"FINAL max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    # print((updated_img - orig_img).mean())\n",
    "    del loss_r_feature_layers, inputs_jit, acts, grads, updated_img\n",
    "    torch.cuda.empty_cache()\n",
    "    # print(\"AFTER memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    # print(\"AFTER memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"AFTER max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    print(time.time() - start, \"seconds\")\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4ab30f0-d8f9-4f4c-bb00-d1e1ab2b03de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "from torch import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ec4d171-758b-4fa8-ab30-79f380dc5f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_acc_list = {}\n",
    "for i in range(num_clients):\n",
    "    new_acc_list[str(i)] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e61a8824-a18c-4f4b-a22f-2289b0cbf9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [], '1': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344efa4e-50ba-4d38-9ba2-5995d415a0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "generating image for client 0\n",
      "BEFORE: max_memory_reserved: 3.593750GB\n",
      "MIDDLE: max_memory_reserved: 3.593750GB\n",
      "FINAL max_memory_reserved: 3.593750GB\n",
      "AFTER max_memory_reserved: 3.593750GB\n",
      "10.405444145202637 seconds\n",
      "generating image for client 1\n",
      "BEFORE: max_memory_reserved: 3.593750GB\n",
      "MIDDLE: max_memory_reserved: 3.593750GB\n",
      "FINAL max_memory_reserved: 3.593750GB\n",
      "AFTER max_memory_reserved: 3.593750GB\n",
      "9.112683773040771 seconds\n",
      "training over\n",
      "1\n",
      "generating image for client 0\n",
      "BEFORE: max_memory_reserved: 3.593750GB\n",
      "MIDDLE: max_memory_reserved: 3.593750GB\n",
      "FINAL max_memory_reserved: 3.593750GB\n",
      "AFTER max_memory_reserved: 3.593750GB\n",
      "10.422072887420654 seconds\n",
      "generating image for client 1\n",
      "BEFORE: max_memory_reserved: 3.593750GB\n",
      "MIDDLE: max_memory_reserved: 3.593750GB\n",
      "FINAL max_memory_reserved: 3.593750GB\n",
      "AFTER max_memory_reserved: 3.593750GB\n",
      "9.109774827957153 seconds\n",
      "training over\n",
      "2\n",
      "generating image for client 0\n",
      "BEFORE: max_memory_reserved: 3.593750GB\n",
      "MIDDLE: max_memory_reserved: 3.593750GB\n",
      "FINAL max_memory_reserved: 3.593750GB\n",
      "AFTER max_memory_reserved: 3.593750GB\n",
      "10.368104457855225 seconds\n",
      "generating image for client 1\n",
      "BEFORE: max_memory_reserved: 3.593750GB\n",
      "MIDDLE: max_memory_reserved: 3.593750GB\n",
      "FINAL max_memory_reserved: 3.593750GB\n",
      "AFTER max_memory_reserved: 3.593750GB\n",
      "9.088015794754028 seconds\n",
      "training over\n",
      "3\n",
      "generating image for client 0\n",
      "BEFORE: max_memory_reserved: 3.593750GB\n",
      "MIDDLE: max_memory_reserved: 3.593750GB\n",
      "FINAL max_memory_reserved: 3.593750GB\n",
      "AFTER max_memory_reserved: 3.593750GB\n",
      "10.350179433822632 seconds\n",
      "generating image for client 1\n",
      "BEFORE: max_memory_reserved: 3.593750GB\n",
      "MIDDLE: max_memory_reserved: 3.593750GB\n",
      "FINAL max_memory_reserved: 3.593750GB\n",
      "AFTER max_memory_reserved: 3.593750GB\n",
      "9.160894393920898 seconds\n",
      "training over\n",
      "4\n",
      "generating image for client 0\n",
      "BEFORE: max_memory_reserved: 3.593750GB\n",
      "MIDDLE: max_memory_reserved: 3.593750GB\n",
      "FINAL max_memory_reserved: 3.593750GB\n",
      "AFTER max_memory_reserved: 3.593750GB\n",
      "10.358952522277832 seconds\n",
      "generating image for client 1\n",
      "BEFORE: max_memory_reserved: 3.593750GB\n",
      "MIDDLE: max_memory_reserved: 3.593750GB\n",
      "FINAL max_memory_reserved: 3.593750GB\n",
      "AFTER max_memory_reserved: 3.593750GB\n",
      "9.17618203163147 seconds\n",
      "training over\n",
      "5\n",
      "generating image for client 0\n",
      "BEFORE: max_memory_reserved: 3.593750GB\n",
      "MIDDLE: max_memory_reserved: 3.593750GB\n",
      "FINAL max_memory_reserved: 3.593750GB\n",
      "AFTER max_memory_reserved: 3.593750GB\n",
      "10.457867622375488 seconds\n",
      "generating image for client 1\n",
      "BEFORE: max_memory_reserved: 3.593750GB\n",
      "MIDDLE: max_memory_reserved: 3.593750GB\n"
     ]
    }
   ],
   "source": [
    "bs = 256\n",
    "kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "fig, axs = plt.subplots(ncols=num_clients, nrows=1, figsize=(22, 3))\n",
    "\n",
    "for epoch in range(500):\n",
    "    collab_data = []\n",
    "    print(epoch)\n",
    "    # Evaluate on the common test set\n",
    "    # for client_num in range(num_clients):\n",
    "    #     c_model = c_models[client_num]\n",
    "    #     c_model.eval()\n",
    "    #     correct, total = 0, 0\n",
    "    #     for x, y_hat in test_loader:\n",
    "    #         with torch.no_grad():\n",
    "    #             x, y_hat = x.to(device), y_hat.to(device)\n",
    "    #             y = c_model(x)\n",
    "    #             correct += (y.argmax(dim=1) == y_hat).sum()\n",
    "    #             total += x.shape[0]\n",
    "    #     acc = correct / total\n",
    "    #     new_acc_list[str(client_num)].append(acc.item())\n",
    "\n",
    "    # Generate collab data\n",
    "    for client_num in range(num_clients):\n",
    "        \"\"\" We generate a zero vector of n (num_classes dimension)\n",
    "        then we generate random numbers within range n and substitute\n",
    "        zero at every index obtained from random number to be 1\n",
    "        This way the zero vector becomes a random one-hot vector\n",
    "        \"\"\"\n",
    "        zeroes = torch.zeros(bs, 10)\n",
    "        ind = torch.randint(low=0, high=10, size=(bs,))\n",
    "        zeroes[torch.arange(start=0, end=bs), ind] = 1\n",
    "        target = zeroes.to(device)\n",
    "        rand_imgs = torch.randn((bs, 3, 32, 32)).to(device)\n",
    "\n",
    "        c_model = c_models[client_num]\n",
    "        c_model.eval()\n",
    "        print(f\"generating image for client {client_num}\")\n",
    "        updated_imgs = run_grad_ascent_on_data(rand_imgs, target, c_model)\n",
    "        with torch.no_grad():\n",
    "            acts = c_model(updated_imgs)[:, :10].detach()\n",
    "        collab_data.append((updated_imgs, acts))\n",
    "        # print(c_models[0].module(updated_imgs).argmax(dim=1))\n",
    "        # print(c_models[1].module(updated_imgs).argmax(dim=1))\n",
    "        # break\n",
    "\n",
    "    # Train each client on their own data and collab data\n",
    "    # for client_num in range(num_clients):\n",
    "    #     c_model = c_models[client_num]\n",
    "    #     c_model.train()\n",
    "    #     c_optim = c_optims[client_num]\n",
    "    #     c_dloader = c_dloaders[client_num]\n",
    "    #     # Train it 10 times on the same distilled dataset\n",
    "    #     for _ in range(1):\n",
    "    #         for c_num, (x, y_hat) in enumerate(collab_data):\n",
    "    #             if c_num == client_num:\n",
    "    #                 # no need to train on its own distilled data\n",
    "    #                 continue\n",
    "    #             x, y_hat = x.to(device), y_hat.to(device)\n",
    "    #             c_optim.zero_grad()\n",
    "    #             y = c_model(x)[:, :10]\n",
    "    #             y = nn.functional.log_softmax(y, dim=1)\n",
    "    #             loss = kl_loss_fn(y, nn.functional.softmax(y_hat, dim=1))\n",
    "    #             loss.backward()\n",
    "    #             c_optim.step()\n",
    "    #     del x, y_hat\n",
    "    #     for x, y_hat in c_dloader:\n",
    "    #         x, y_hat = x.to(device), y_hat.to(device)\n",
    "    #         c_optim.zero_grad()\n",
    "    #         y = c_model(x)\n",
    "    #         loss = loss_fn(y, y_hat)\n",
    "    #         loss.backward()\n",
    "    #         c_optim.step()\n",
    "    print(\"training over\")\n",
    "    del collab_data, updated_imgs, acts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5a138-744a-4757-846a-8b3fd7bcd6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf3683-a029-41c8-83db-0a92906dc9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
